{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"style_transfer.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"hy31D7gzprN1","colab_type":"text"},"cell_type":"markdown","source":["# Google Colab 에서 Style Transfer 해보기 \n","\n","## [Original Code](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/neural_style_transfer)"]},{"metadata":{"id":"O29Jh3WxVzmc","colab_type":"code","colab":{}},"cell_type":"code","source":["## CUDA 8\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n","!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8N5A7deilV1Q","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","print(torch.__version__)\n","print(torch.cuda.is_available())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-98tgd5nXUE7","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","!pip3 install torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"57L4zeoJXXiK","colab_type":"code","colab":{}},"cell_type":"code","source":["import sys\n","sys.version\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nm0uFlEeX2Zi","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","!cat /etc/issue.net"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lz81zNSFYMwh","colab_type":"code","colab":{}},"cell_type":"code","source":["!cat /proc/cpuinfo"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NMT1ktXnYPYJ","colab_type":"code","colab":{}},"cell_type":"code","source":["!cat /proc/meminfo"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_N2_RRepYTBI","colab_type":"code","colab":{}},"cell_type":"code","source":["!df -h"],"execution_count":0,"outputs":[]},{"metadata":{"id":"65Ns0BAIYVKZ","colab_type":"code","colab":{}},"cell_type":"code","source":["!nvidia-smi"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aXab3QgUYcAx","colab_type":"code","colab":{}},"cell_type":"code","source":["## Google Drive 와 연동하기\n","\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d-rhbmqwYx4v","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j-bYU08LYyIv","colab_type":"code","colab":{}},"cell_type":"code","source":["!cd drive/Colab\\ Notebooks/Pytorch/style\\ transfer/ ; ls -al"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wwX618WiYyLX","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","os.chdir(\"drive/Colab Notebooks/Pytorch/style transfer\")\n","os.getcwd()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GwPxJMDtb3hK","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls -al"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GMktyBITka2i","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","print(torch.cuda.is_available())\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_lYpG4SIw7Sq","colab_type":"text"},"cell_type":"markdown","source":["    ## 변경 가능한 부분 \n","\n","```\n","    parser.add_argument('--content', type=str, default='content.png')\n","    parser.add_argument('--style', type=str, default='style.png')\n","    parser.add_argument('--max_size', type=int, default=400)\n","    parser.add_argument('--total_step', type=int, default=2000)\n","    parser.add_argument('--log_step', type=int, default=10)\n","    parser.add_argument('--sample_step', type=int, default=500)\n","    parser.add_argument('--style_weight', type=float, default=100)\n","    parser.add_argument('--lr', type=float, default=0.003)\n","```\n","\n"]},{"metadata":{"id":"AXE-0JkfYyN2","colab_type":"code","colab":{}},"cell_type":"code","source":["## 원래 이렇게 하면 되야하는데 돌아가야하는데 안돌아감 => 잘돌아감\n","!python3 main.py --content='content3.jpg' --style='style9.jpg' --total_step=5000  --style_weight=3000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OZNW9ckH2Q7S","colab_type":"code","colab":{}},"cell_type":"code","source":["!python3 main.py --content='content2.jpg' --style='style4.jpg' --total_step=4000 --style_weight=5000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YhToax6Davh8","colab_type":"code","colab":{}},"cell_type":"code","source":["## main.py 있는 값을 빼와서 해보기 \n","from __future__ import division\n","from torchvision import models\n","from torchvision import transforms\n","from PIL import Image\n","import argparse\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KnE7zn-CbMfD","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","\n","def load_image(image_path, transform=None, max_size=None, shape=None):\n","    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n","    image = Image.open(image_path)\n","    \n","    if max_size:\n","        scale = max_size / max(image.size)\n","        size = np.array(image.size) * scale\n","        image = image.resize(size.astype(int), Image.ANTIALIAS)\n","    \n","    if shape:\n","        image = image.resize(shape, Image.LANCZOS)\n","    \n","    if transform:\n","        image = transform(image).unsqueeze(0)\n","    \n","    return image.to(device)\n","\n","\n","class VGGNet(nn.Module):\n","    def __init__(self):\n","        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n","        super(VGGNet, self).__init__()\n","        self.select = ['0', '5', '10', '19', '28'] \n","        self.vgg = models.vgg19(pretrained=True).features\n","        \n","    def forward(self, x):\n","        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n","        features = []\n","        for name, layer in self.vgg._modules.items():\n","            x = layer(x)\n","            if name in self.select:\n","                features.append(x)\n","        return features\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"utikdD-EbKgS","colab_type":"code","colab":{}},"cell_type":"code","source":["def main( content_filename , style_filename ):\n","    \n","    # Image preprocessing\n","    # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n","    # We use the same normalization statistics here.\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n","                             std=(0.229, 0.224, 0.225))])\n","    \n","    # Load content and style images\n","    # Make the style image same size as the content image\n","    content = load_image(content_filename , transform, max_size= 400 )\n","    style = load_image(style_filename , transform, shape=[content.size(2), content.size(3)])\n","    \n","    # Initialize a target image with the content image\n","    target = content.clone().requires_grad_(True)\n","    \n","    optimizer = torch.optim.Adam([target], lr=0.003, betas=[0.5, 0.999])\n","    vgg = VGGNet().to(device).eval()\n","    \n","    for step in range(2000):\n","        \n","        # Extract multiple(5) conv feature vectors\n","        target_features = vgg(target)\n","        content_features = vgg(content)\n","        style_features = vgg(style)\n","\n","        style_loss = 0\n","        content_loss = 0\n","        for f1, f2, f3 in zip(target_features, content_features, style_features):\n","            # Compute content loss with target and content images\n","            content_loss += torch.mean((f1 - f2)**2)\n","\n","            # Reshape convolutional feature maps\n","            _, c, h, w = f1.size()\n","            f1 = f1.view(c, h * w)\n","            f3 = f3.view(c, h * w)\n","\n","            # Compute gram matrix\n","            f1 = torch.mm(f1, f1.t())\n","            f3 = torch.mm(f3, f3.t())\n","\n","            # Compute style loss with target and style images\n","            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n","        \n","        # Compute total loss, backprop and optimize\n","        loss = content_loss + 100  * style_loss \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (step+1) % 10 == 0:\n","            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n","                   .format(step+1, 2000 , content_loss.item(), style_loss.item()))\n","\n","        if (step+1) % 500 == 0:\n","            # Save the generated image\n","            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n","            img = target.clone().squeeze()\n","            img = denorm(img).clamp_(0, 1)\n","            torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kH0uDyYfcPmi","colab_type":"code","colab":{}},"cell_type":"code","source":["main(content_filename = \"content.jpg\" , style_filename  =  \"style.jpg\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"izm9jGy0f9Mj","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls -al"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K-NSC5zKk88z","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}